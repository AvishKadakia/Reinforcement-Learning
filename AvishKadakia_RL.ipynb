{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "functional-basics",
   "metadata": {},
   "source": [
    "   # Method 1: Using DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "female-number",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "allied-carolina",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.optimizers import Adam, RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "economic-circular",
   "metadata": {},
   "source": [
    "## Defining Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "mechanical-employer",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training parameters\n",
    "n_episodes = 300\n",
    "n_win_ticks = 200\n",
    "\n",
    "gamma = 1.0 # Discount Factor\n",
    "epsilon = 1.0 # Exploration Factor\n",
    "epsilon_decay = 0.99\n",
    "epsilon_min = 0.01\n",
    "lr = 0.01 #learning rate\n",
    "lr_decay = 0.01\n",
    "\n",
    "batch_size = 64 # how may samples to train on from memmory\n",
    "monitor = False\n",
    "quiet = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eleven-facial",
   "metadata": {},
   "source": [
    "## Setting up the Cart Pole environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bearing-atmosphere",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Parameter\n",
    "memory = deque(maxlen=10000)\n",
    "env = gym.make('CartPole-v0')\n",
    "env.max_episode_steps = 500\n",
    "input_shape = 4\n",
    "action_space = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detailed-healing",
   "metadata": {},
   "source": [
    "## Neural Network Architechture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fitting-conservative",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OurModel(input_shape, action_space):\n",
    "    # Input Layer of state size(4)\n",
    "    X_input = Input(input_shape)\n",
    "    # Hidden Layer with 512 nodes\n",
    "    X = Dense(512, input_shape=input_shape, activation=\"relu\")(X_input)\n",
    "    # Hidden layer with 256 nodes\n",
    "    X = Dense(256, activation=\"relu\")(X)\n",
    "    # Hidden layer with 64 nodes\n",
    "    X = Dense(64, activation=\"relu\")(X)\n",
    "    # Output Layer with # of actions: 2 nodes (left, right)\n",
    "    X = Dense(action_space, activation=\"linear\")(X)\n",
    "    model = Model(inputs = X_input, outputs = X, name='CartPole_DQN_model')\n",
    "    model.compile(loss=\"mse\", optimizer=RMSprop(lr=0.00025, rho=0.95, epsilon=0.01), metrics=[\"accuracy\"])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "figured-intention",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "treated-domestic",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self):\n",
    "        #Setting Up environment and initialising parameters\n",
    "        self.env = gym.make('CartPole-v1')\n",
    "        self.state_size = self.env.observation_space.shape[0]\n",
    "        self.action_size = self.env.action_space.n\n",
    "        self.EPISODES = 1000\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.001\n",
    "        self.epsilon_decay = 0.999\n",
    "        self.batch_size = 64\n",
    "        self.train_start = 1000\n",
    "        # creating main model\n",
    "        self.model = OurModel(input_shape=(self.state_size,), action_space = self.action_size)\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        if len(self.memory) > self.train_start:\n",
    "            if self.epsilon > self.epsilon_min:\n",
    "                self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def select_action(self, state):\n",
    "        if np.random.random() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            return np.argmax(self.model.predict(state))\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return\n",
    "        # Randomly sample minibatch from the memory and then taining neural network on the experience\n",
    "        minibatch = random.sample(self.memory, min(len(self.memory), self.batch_size))\n",
    "        state = np.zeros((self.batch_size, self.state_size))\n",
    "        next_state = np.zeros((self.batch_size, self.state_size))\n",
    "        action, reward, done = [], [], []\n",
    "        for i in range(self.batch_size):\n",
    "            state[i] = minibatch[i][0]\n",
    "            action.append(minibatch[i][1])\n",
    "            reward.append(minibatch[i][2])\n",
    "            next_state[i] = minibatch[i][3]\n",
    "            done.append(minibatch[i][4])\n",
    "        target = self.model.predict(state)\n",
    "        target_next = self.model.predict(next_state)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            # Updating Q value for the action\n",
    "            if done[i]:\n",
    "                target[i][action[i]] = reward[i]\n",
    "            else:\n",
    "                target[i][action[i]] = reward[i] + self.gamma * (np.amax(target_next[i]))\n",
    "\n",
    "        # Train the Neural Network with batches\n",
    "        self.model.fit(state, target, batch_size=self.batch_size, verbose=0)\n",
    "\n",
    "    def run(self):\n",
    "        flag = 0\n",
    "        for e in range(self.EPISODES):\n",
    "            state = self.env.reset()\n",
    "            state = np.reshape(state, [1, self.state_size])\n",
    "            done = False\n",
    "            i = 0\n",
    "            while not done:\n",
    "                #self.env.render()\n",
    "                action = self.select_action(state)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                next_state = np.reshape(next_state, [1, self.state_size])\n",
    "                if not done or i == self.env._max_episode_steps-1:\n",
    "                    self.remember(state, action, reward, next_state, done)\n",
    "                else:\n",
    "                    self.remember(state, action, -100, next_state, done)\n",
    "                \n",
    "                state = next_state\n",
    "                i += reward\n",
    "                if done:       \n",
    "                               \n",
    "                    print(f\"episode: {e}/{self.EPISODES}, score: {i}, e: {self.epsilon}\")\n",
    "                    if i >= 200:\n",
    "                        print(\"|----------------------------------Solved----------------------------------|\")\n",
    "                        print(f\"episode: {e}/{self.EPISODES}, score: {i}, e: {self.epsilon}\")\n",
    "                        flag = 1\n",
    "                        break\n",
    "                    if flag == 1:\n",
    "                      break\n",
    "                if flag == 1:\n",
    "                      break\n",
    "                self.replay()\n",
    "            if flag == 1:\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mounted-orlando",
   "metadata": {},
   "source": [
    "## Executing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "collectible-austin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Method 1 Using DQN-------\n",
      "Model: \"CartPole_DQN_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 4)]               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               2560      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 150,466\n",
      "Trainable params: 150,466\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "episode: 0/1000, score: 52.0, e: 1.0\n",
      "episode: 1/1000, score: 28.0, e: 1.0\n",
      "episode: 2/1000, score: 17.0, e: 1.0\n",
      "episode: 3/1000, score: 12.0, e: 1.0\n",
      "episode: 4/1000, score: 14.0, e: 1.0\n",
      "episode: 5/1000, score: 14.0, e: 1.0\n",
      "episode: 6/1000, score: 21.0, e: 1.0\n",
      "episode: 7/1000, score: 11.0, e: 1.0\n",
      "episode: 8/1000, score: 12.0, e: 1.0\n",
      "episode: 9/1000, score: 22.0, e: 1.0\n",
      "episode: 10/1000, score: 26.0, e: 1.0\n",
      "episode: 11/1000, score: 16.0, e: 1.0\n",
      "episode: 12/1000, score: 24.0, e: 1.0\n",
      "episode: 13/1000, score: 17.0, e: 1.0\n",
      "episode: 14/1000, score: 19.0, e: 1.0\n",
      "episode: 15/1000, score: 19.0, e: 1.0\n",
      "episode: 16/1000, score: 16.0, e: 1.0\n",
      "episode: 17/1000, score: 15.0, e: 1.0\n",
      "episode: 18/1000, score: 13.0, e: 1.0\n",
      "episode: 19/1000, score: 16.0, e: 1.0\n",
      "episode: 20/1000, score: 10.0, e: 1.0\n",
      "episode: 21/1000, score: 15.0, e: 1.0\n",
      "episode: 22/1000, score: 31.0, e: 1.0\n",
      "episode: 23/1000, score: 20.0, e: 1.0\n",
      "episode: 24/1000, score: 46.0, e: 1.0\n",
      "episode: 25/1000, score: 47.0, e: 1.0\n",
      "episode: 26/1000, score: 21.0, e: 1.0\n",
      "episode: 27/1000, score: 41.0, e: 1.0\n",
      "episode: 28/1000, score: 25.0, e: 1.0\n",
      "episode: 29/1000, score: 23.0, e: 1.0\n",
      "episode: 30/1000, score: 22.0, e: 1.0\n",
      "episode: 31/1000, score: 19.0, e: 1.0\n",
      "episode: 32/1000, score: 20.0, e: 1.0\n",
      "episode: 33/1000, score: 18.0, e: 1.0\n",
      "episode: 34/1000, score: 17.0, e: 1.0\n",
      "episode: 35/1000, score: 10.0, e: 1.0\n",
      "episode: 36/1000, score: 38.0, e: 1.0\n",
      "episode: 37/1000, score: 29.0, e: 1.0\n",
      "episode: 38/1000, score: 12.0, e: 1.0\n",
      "episode: 39/1000, score: 11.0, e: 1.0\n",
      "episode: 40/1000, score: 12.0, e: 1.0\n",
      "episode: 41/1000, score: 19.0, e: 1.0\n",
      "episode: 42/1000, score: 56.0, e: 1.0\n",
      "episode: 43/1000, score: 14.0, e: 1.0\n",
      "episode: 44/1000, score: 11.0, e: 1.0\n",
      "episode: 45/1000, score: 44.0, e: 0.9851045463620021\n",
      "episode: 46/1000, score: 18.0, e: 0.9675225846837673\n",
      "episode: 47/1000, score: 27.0, e: 0.9417362622231683\n",
      "episode: 48/1000, score: 45.0, e: 0.9002772252562138\n",
      "episode: 49/1000, score: 17.0, e: 0.88509434007808\n",
      "episode: 50/1000, score: 14.0, e: 0.8727832416118043\n",
      "episode: 51/1000, score: 11.0, e: 0.8632304853107438\n",
      "episode: 52/1000, score: 16.0, e: 0.8495219033622532\n",
      "episode: 53/1000, score: 24.0, e: 0.8293661352855802\n",
      "episode: 54/1000, score: 18.0, e: 0.8145637636371417\n",
      "episode: 55/1000, score: 19.0, e: 0.7992255563671304\n",
      "episode: 56/1000, score: 40.0, e: 0.7678721062162944\n",
      "episode: 57/1000, score: 14.0, e: 0.7571914943525904\n",
      "episode: 58/1000, score: 44.0, e: 0.724581445483085\n",
      "episode: 59/1000, score: 32.0, e: 0.7017506636113059\n",
      "episode: 60/1000, score: 56.0, e: 0.6635141250307047\n",
      "episode: 61/1000, score: 20.0, e: 0.6503691570122084\n",
      "episode: 62/1000, score: 84.0, e: 0.5979446000009478\n",
      "episode: 63/1000, score: 81.0, e: 0.5513983909676525\n",
      "episode: 64/1000, score: 72.0, e: 0.5130747553488376\n",
      "episode: 65/1000, score: 30.0, e: 0.4979036311114436\n",
      "episode: 66/1000, score: 82.0, e: 0.4586858344239834\n",
      "episode: 67/1000, score: 119.0, e: 0.4072006165777428\n",
      "episode: 68/1000, score: 101.0, e: 0.36806348825922275\n",
      "episode: 69/1000, score: 131.0, e: 0.32285067248442284\n",
      "episode: 70/1000, score: 168.0, e: 0.27290011414765825\n",
      "episode: 71/1000, score: 132.0, e: 0.23913776344553783\n",
      "episode: 72/1000, score: 127.0, e: 0.21060329799922556\n",
      "episode: 73/1000, score: 332.0, e: 0.15108009835289823\n",
      "|----------------------------------Solved----------------------------------|\n",
      "episode: 73/1000, score: 332.0, e: 0.15108009835289823\n"
     ]
    }
   ],
   "source": [
    "print(\"----------Method 1: Using DQN-------\")\n",
    "agent = DQNAgent()\n",
    "agent.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facial-norman",
   "metadata": {},
   "source": [
    "# Method 2: Using NN with Genetic Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "missing-disposal",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "improving-format",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "from random import randint\n",
    "from statistics import median, mean\n",
    "np.random.seed(seed=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hydraulic-leadership",
   "metadata": {},
   "source": [
    "## Settingup Initial Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bottom-reggae",
   "metadata": {},
   "outputs": [],
   "source": [
    "award_set =[]\n",
    "test_run = 15\n",
    "best_gen =[]\n",
    "n_of_generations = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grateful-converter",
   "metadata": {},
   "source": [
    "## Setting Up Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "sporting-transformation",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "ind = env.observation_space.shape[0]\n",
    "adim = env.action_space.n #discrete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compound-words",
   "metadata": {},
   "source": [
    "## Creating Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "logical-communications",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    x = np.exp(x)/np.sum(np.exp(x))\n",
    "    return x\n",
    "\n",
    "def lreLu(x):\n",
    "    alpha=0.2\n",
    "    return tf.nn.relu(x)-alpha*tf.nn.relu(-x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def reLu(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "def nn(obs,in_w,in_b,hid_w,out_w):\n",
    "\n",
    "    obs = obs/max(np.max(np.linalg.norm(obs)),1) \n",
    "\n",
    "    Ain = reLu(np.dot(obs,in_w)+in_b.T)\n",
    "\n",
    "    Ahid = reLu(np.dot(Ain,hid_w))\n",
    "    lhid = np.dot(Ahid,out_w)\n",
    "\n",
    "    out_put = reLu(lhid)\n",
    "    out_put = softmax(out_put)\n",
    "    out_put = out_put.argsort().reshape(1,adim)\n",
    "    act = out_put[0][0] #index of discrete action\n",
    "\n",
    "    return act"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "employed-fellowship",
   "metadata": {},
   "source": [
    "## Generate initial set of weights and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "mounted-reducing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intial_gen(test_run):\n",
    "    input_weight = []\n",
    "    input_bias = []\n",
    "\n",
    "    hidden_weight = []\n",
    "    out_weight = [] \n",
    "\n",
    "    in_node = 4  \n",
    "    hid_node = 2\n",
    "\n",
    "    for i in range(test_run):\n",
    "        in_w = np.random.rand(ind,in_node)\n",
    "        input_weight.append(in_w)\n",
    "\n",
    "        in_b = np.random.rand((in_node))\n",
    "        input_bias.append(in_b)\n",
    "\n",
    "        hid_w = np.random.rand(in_node,hid_node)\n",
    "        hidden_weight.append(hid_w)\n",
    "\n",
    "\n",
    "        out_w = np.random.rand(hid_node, adim)\n",
    "        out_weight.append(out_w)\n",
    "\n",
    "    generation = [input_weight, input_bias, hidden_weight, out_weight]\n",
    "    return generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conscious-pledge",
   "metadata": {},
   "source": [
    "## Run environment randomly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "blocked-music",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rand_run(env,test_run):\n",
    "    award_set = []\n",
    "    generations = intial_gen(test_run)\n",
    "\n",
    "    for episode in range(test_run):# run env 10 time\n",
    "        in_w  = generations[0][episode]\n",
    "        in_b = generations[1][episode]\n",
    "        hid_w =  generations[2][episode]\n",
    "        out_w =  generations[3][episode]\n",
    "        award = run_env(env,in_w,in_b,hid_w,out_w)\n",
    "        award_set = np.append(award_set,award)\n",
    "    gen_award = [generations, award_set]\n",
    "    return gen_award "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustainable-geography",
   "metadata": {},
   "source": [
    "## Genetic Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "unable-leonard",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_env(env,in_w,in_b,hid_w,out_w):\n",
    "    obs = env.reset()\n",
    "    award = 0\n",
    "    for t in range(300):\n",
    "        #env.render() this slows the process theredore commented\n",
    "        action = nn(obs,in_w,in_b,hid_w,out_w)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        award += reward \n",
    "        if done:\n",
    "            break\n",
    "    return award\n",
    "\n",
    "def mutation(new_dna):\n",
    "\n",
    "    j = np.random.randint(0,len(new_dna))\n",
    "    if ( 0 <j < 10): # controlling rate for amount of mutation\n",
    "        for ix in range(j):\n",
    "            n = np.random.randint(0,len(new_dna)) #random postion for mutation\n",
    "            new_dna[n] = new_dna[n] + np.random.rand()\n",
    "\n",
    "    mut_dna = new_dna\n",
    "\n",
    "    return mut_dna\n",
    "\n",
    "def crossover(Dna_list):\n",
    "    newDNA_list = []\n",
    "    newDNA_list.append(Dna_list[0])\n",
    "    newDNA_list.append(Dna_list[1]) \n",
    "\n",
    "    for l in range(10):  # generation after crassover\n",
    "        j = np.random.randint(0,len(Dna_list[0]))\n",
    "        new_dna = np.append(Dna_list[0][:j], Dna_list[1][j:])\n",
    "\n",
    "        mut_dna = mutation(new_dna)\n",
    "        newDNA_list.append(mut_dna)\n",
    "\n",
    "    return newDNA_list\n",
    "\n",
    "#Generate new set of weights and bias from the best previous weights and bias\n",
    "\n",
    "def reproduce(award_set, generations):\n",
    "\n",
    "    good_award_idx = award_set.argsort()[-2:][::-1] # here only best 2 are selected \n",
    "    good_generation = []\n",
    "    DNA_list = []\n",
    "\n",
    "    new_input_weight = []\n",
    "    new_input_bias = []\n",
    "\n",
    "    new_hidden_weight = []\n",
    "\n",
    "    new_output_weight =[]\n",
    "\n",
    "    new_award_set = []\n",
    "\n",
    "\n",
    "    #Extraction of all weight info into a single sequence\n",
    "    for index in good_award_idx:\n",
    "\n",
    "        w1 = generations[0][index]\n",
    "        dna_in_w = w1.reshape(w1.shape[1],-1)\n",
    "\n",
    "        b1 = generations[1][index]\n",
    "        dna_b1 = np.append(dna_in_w, b1)\n",
    "\n",
    "        w2 = generations[2][index]\n",
    "        dna_whid = w2.reshape(w2.shape[1],-1)\n",
    "        dna_w2 = np.append(dna_b1,dna_whid)\n",
    "\n",
    "        wh = generations[3][index]\n",
    "        dna = np.append(dna_w2, wh)\n",
    "\n",
    "\n",
    "        DNA_list.append(dna) # make 2 dna for good gerneration\n",
    "\n",
    "    newDNA_list = crossover(DNA_list)\n",
    "\n",
    "    for newdna in newDNA_list: # collection of weights from dna info\n",
    "\n",
    "        newdna_in_w1 = np.array(newdna[:generations[0][0].size]) \n",
    "        new_in_w = np.reshape(newdna_in_w1, (-1,generations[0][0].shape[1]))\n",
    "        new_input_weight.append(new_in_w)\n",
    "\n",
    "        new_in_b = np.array([newdna[newdna_in_w1.size:newdna_in_w1.size+generations[1][0].size]]).T #bias\n",
    "        new_input_bias.append(new_in_b)\n",
    "\n",
    "        sh = newdna_in_w1.size + new_in_b.size\n",
    "        newdna_in_w2 = np.array([newdna[sh:sh+generations[2][0].size]])\n",
    "        new_hid_w = np.reshape(newdna_in_w2, (-1,generations[2][0].shape[1]))\n",
    "        new_hidden_weight.append(new_hid_w)\n",
    "\n",
    "        sl = newdna_in_w1.size + new_in_b.size + newdna_in_w2.size\n",
    "        new_out_w   = np.array([newdna[sl:]]).T\n",
    "        new_out_w = np.reshape(new_out_w, (-1,generations[3][0].shape[1]))\n",
    "        new_output_weight.append(new_out_w)\n",
    "\n",
    "        new_award = run_env(env, new_in_w, new_in_b, new_hid_w, new_out_w) #bias\n",
    "        new_award_set = np.append(new_award_set,new_award)\n",
    "\n",
    "    new_generation = [new_input_weight,new_input_bias,new_hidden_weight,new_output_weight]\n",
    "\n",
    "    return new_generation, new_award_set\n",
    "\n",
    "\n",
    "def evolution(env,test_run,n_of_generations):\n",
    "    gen_award = rand_run(env, test_run)\n",
    "    current_gens = gen_award[0] \n",
    "    current_award_set = gen_award[1]\n",
    "    best_gen =[]\n",
    "    A =[]\n",
    "    for n in range(n_of_generations):\n",
    "        new_generation, new_award_set = reproduce(current_award_set, current_gens)\n",
    "        current_gens = new_generation\n",
    "        current_award_set = new_award_set\n",
    "        avg = np.average(current_award_set)\n",
    "        a = np.amax(current_award_set)\n",
    "        print(f\"generation: {n+1}, score: {a}\")\n",
    "        if np.amax(current_award_set) >= 200:\n",
    "            print(\"|----------------------------------Solved----------------------------------|\")\n",
    "            print(f\"generation: {n}/{n_of_generations}, score: {np.amax(current_award_set)}\")\n",
    "            break\n",
    "        \n",
    "        A = np.append(A, a)\n",
    "\n",
    "    Best_award = np.amax(A)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "large-vegetation",
   "metadata": {},
   "source": [
    "## Executing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "experienced-windows",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Method 2 Using NN with Genetic Algorithm-------\n",
      "generation: 1, score: 10.0\n",
      "generation: 2, score: 27.0\n",
      "generation: 3, score: 37.0\n",
      "generation: 4, score: 110.0\n",
      "generation: 5, score: 109.0\n",
      "generation: 6, score: 107.0\n",
      "generation: 7, score: 160.0\n",
      "generation: 8, score: 144.0\n",
      "generation: 9, score: 158.0\n",
      "generation: 10, score: 150.0\n",
      "generation: 11, score: 177.0\n",
      "generation: 12, score: 176.0\n",
      "generation: 13, score: 300.0\n",
      "|----------------------------------Solved----------------------------------|\n",
      "generation: 12/1000, score: 300.0\n"
     ]
    }
   ],
   "source": [
    "print(\"----------Method 2: Using NN with Genetic Algorithm-------\")\n",
    "\n",
    "evolution(env, test_run, n_of_generations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
